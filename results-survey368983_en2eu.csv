Response ID,Date submitted,Last page,Start language,Seed,Date started,Date last action,Team name,Please select the language pair for your run:,Which translation method did you use in your selected run?,Which translation method did you use in your selected run? [Other],Which type of NMT did you use?,Which type of NMT did you use? [Comment],Did you use a specific library or toolkit for your system?,Did you use a specific library or toolkit for your system? [Comment],Was your translation model: [Trained from scratch],Was your translation model: [Fine-tuned],Did you use parallel in-domain training data?,Did you use parallel in-domain training data? [Comment],What was the size of the in-domain parallel training corpus? Please enter the number of aligned parallel segments.,Did you use open domain parallel training data?,Did you use open domain parallel training data? [Comment],What was the size of the open domain parallel training corpus? Please enter the number of aligned parallel segments.,Did you use monolingual in-domain training data?,Was the in-domain monolingual training data: [For the source language? Please supply details on the dataset including size in number of segments in the comment box.],Was the in-domain monolingual training data: [Comment],Was the in-domain monolingual training data: [For the target language? Please supply details on the dataset including size in number of segments in the comment box.],Was the in-domain monolingual training data: [Comment],Did you use open domain monolingual training data?,Was the open domain monolingual training dataset: [For the source language? Please supply details on the dataset including size in number of segments in the comment box.],Was the open domain monolingual training dataset: [Comment],Was the open domain monolingual training dataset: [For the target language? Please supply details on the dataset including size in number of segments in the comment box.],Was the open domain monolingual training dataset: [Comment],Did you use a validation data set?,Did you use a validation data set? [Other],Did you use Back Translation?,Did you use pre-trained language models?,Did you use pre-trained language models? [Comment],Total time,Group time: Run information,Question time: Q1,Question time: Q2,Group time: Translation Method,Question time: Q3,Question time: Q4,Question time: Q5,Question time: Q10,Group time: Training Data,Question time: Q6,Question time: Q7,Question time: Q8,Question time: Q9,Question time: Q11,Question time: Q13,Question time: Q12,Question time: Q14,Question time: Q17,Question time: Q15,Question time: Q16
29,2020-07-15 11:40:46,3,en,913542811,2020-07-15 11:29:05,2020-07-15 11:40:46,DCU-MT,en2eu (English to Basque),Neural Machine Translation (NMT),,transformer,,"Yes (Please enter details in the comment box, e.g. ""Marian NMT with sequence2sequence...""",Marian NMT,Yes,No,No,We made use of in-domain and out-of-domain parallel data provided by the organisers,,No,We made use of in-domain and out-of-domain parallel data provided by the organisers,,Yes,Yes,200K biomedical domain data selected by TermFinder,Yes,"442K moxed (biomedical 41,151 and rest out-of-domain data selected by TermFinder)",Yes,No,,Yes,We sued CommonCrawl corpus,Other,"Yes split the test set into parts, one used fro validation and another part was used for test set",Yes,No,,703.3,115.83,,,59.71,,,,,527.76,,,,,,,,,,,
36,2020-07-15 12:54:37,3,en,162856814,2020-07-15 12:22:00,2020-07-15 12:54:37,Elhuyar_NLP_team,en2eu (English to Basque),Neural Machine Translation (NMT),,transformer,,"Yes (Please enter details in the comment box, e.g. ""Marian NMT with sequence2sequence...""",OpenNMT toolkit with transformers,No,Yes,"Yes (Please enter details in the comment box, e.g. ""MEDLINE corpus supplied by WMT biomedical task organizers""","In domain data has been collected from different sources: WMT20 shared task bilingual training data, Elhuyarâ€™s internal medical corpus extracted from translation memories and synthetically generated data from the WMT19 EN-ES shared task.",Around 350k segments,"Yes (Please enter details in the comment box, e.g. ""Corpus supplied by the WMT 2020 News task organizers""",An internal synthetic corpus has been used. Synthetic data was obtained by backtranslating an internal ES-EU corpus from Spanish to English,Around 7M segments,Yes,No,,Yes,"Shared task monolingual in-domian data for Basque comprising SNOMED descriptions, hospital notes and wikipedia medical articles. Data has been cleaned and processed. Around 110k segments",No,N/A,,N/A,,Other,A mix of in-domain and open domain data.  In-domain data consists of an internal validation set and shared task validation set. Open domain is a split of the open domain training data.,Yes,No,,1959.37,44.67,,,543.78,,,,,1370.92,,,,,,,,,,,
38,,1,en,72070010,2020-07-15 12:55:19,2020-07-15 12:57:55,Elhuyar_NLP_team,en2eu (English to Basque),,,,,,,N/A,N/A,,,,,,,,N/A,,N/A,,,N/A,,N/A,,,,,,,156,156,,,,,,,,,,,,,,,,,,,
55,2020-07-16 02:09:27,3,en,1039279243,2020-07-16 01:43:03,2020-07-16 02:09:27,UTS_NLP,en2eu (English to Basque),Neural Machine Translation (NMT),,transformer,BERT-FUSED NMT (based on transformers),"Yes (Please enter details in the comment box, e.g. ""Marian NMT with sequence2sequence...""",bert-nmt (based on fairseq),Yes,No,"Yes (Please enter details in the comment box, e.g. ""MEDLINE corpus supplied by WMT biomedical task organizers""",ICD-10 codes translations.,25900,"Yes (Please enter details in the comment box, e.g. ""Corpus supplied by the WMT 2020 News task organizers""",Out of domain parallel corpora provided by WMT2020 biomedical translation organizers.,~0.6M,Yes,No,,Yes,"Snomed terms, hospital notes and wikipedia medical articles (total of 60,000 sentences)",Yes,No,,Yes,Wikipedia 1.5M,Other,For the terminology transation I used the validation set provided by the organizers. For the abstract translation I used a small set of sentences translated myself from in-domain English sentences.,Yes,"Yes (Please enter details in the comment box, e.g. ""multilingual BERT"")","I used pretrained BERT models from HuggingFace:
- ""ixa-ehu/berteus-base-cased"" for back-translation with the BERT-fused NMT.
- ""adamlin/NCBI_BERT_pubmed_mimic_uncased_base_transformers"" for the translations with the BERT-fused NMT.
",1585.08,54.55,,,99.35,,,,,1431.18,,,,,,,,,,,
73,,2,en,1804577341,2020-07-16 15:51:21,2020-07-16 15:53:50,Ixamed,en2eu (English to Basque),Neural Machine Translation (NMT),,transformer,,"Yes (Please enter details in the comment box, e.g. ""Marian NMT with sequence2sequence...""",OpenNMT with Transformer,Yes,No,,,,,,,,N/A,,N/A,,,N/A,,N/A,,,,,,,149.03,92.33,,,56.7,,,,,,,,,,,,,,,,
